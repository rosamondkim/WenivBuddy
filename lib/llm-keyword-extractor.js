/**
 * LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
 * OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ 
 * í•œì˜ ë³€í™˜, ì˜¤íƒ€ ìˆ˜ì •, ì»¨í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */

/**
 * LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
 * @param {string} question - ì‚¬ìš©ì ì§ˆë¬¸
 * @returns {Promise<Object|null>} - ì¶”ì¶œ ê²°ê³¼ ë˜ëŠ” null (ì‹¤íŒ¨ ì‹œ)
 */
export async function extractKeywordsWithLLM(question) {
  if (!question || typeof question !== 'string') {
    console.warn('âš ï¸ Invalid question for LLM extraction')
    return null
  }

  try {
    console.log('ğŸ¤– [LLM] Calling API for keyword extraction...')

    const response = await fetch('/api/extract-keywords', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ question })
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      console.error('âŒ [LLM] API error:', response.status, errorData)

      // API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
      if (response.status === 500 && errorData.error?.includes('API key')) {
        console.warn('âš ï¸ [LLM] OpenAI API key not configured. Falling back to local extraction.')
        return null
      }

      throw new Error(errorData.error || `API error: ${response.status}`)
    }

    const data = await response.json()

    console.log('âœ… [LLM] Successfully extracted keywords:', data.keywords)

    return {
      keywords: data.keywords || [],
      category: data.category || 'all',
      corrected_terms: data.corrected_terms || {},
      source: 'llm',
      model: data.model || 'gpt-4o-mini',
      usage: data.usage
    }

  } catch (error) {
    console.error('âŒ [LLM] Extraction failed:', error.message)
    return null
  }
}

/**
 * LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
 * @returns {Promise<boolean>} - ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ true
 */
export async function isLLMAvailable() {
  try {
    const response = await fetch('/api/extract-keywords', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ question: 'test' })
    })

    return response.ok
  } catch (error) {
    return false
  }
}

/**
 * LLM í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
 * @param {string} question - ì‚¬ìš©ì ì§ˆë¬¸
 * @returns {Object} - ì˜ˆìƒ í† í° ìˆ˜ ë° ë¹„ìš©
 */
export function estimateLLMCost(question) {
  // ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (ì˜ë¬¸ 4ì â‰ˆ 1í† í°, í•œê¸€ 1.5ì â‰ˆ 1í† í°)
  const questionTokens = Math.ceil(question.length / 3)
  const systemPromptTokens = 100 // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
  const responseTokens = 100 // ì‘ë‹µ JSON

  const inputTokens = questionTokens + systemPromptTokens
  const outputTokens = responseTokens

  // GPT-4o-mini ê°€ê²© (2024ë…„ ê¸°ì¤€)
  const inputCost = (inputTokens / 1_000_000) * 0.150 // $0.150 per 1M tokens
  const outputCost = (outputTokens / 1_000_000) * 0.600 // $0.600 per 1M tokens
  const totalCost = inputCost + outputCost

  return {
    inputTokens,
    outputTokens,
    totalTokens: inputTokens + outputTokens,
    estimatedCost: totalCost,
    estimatedCostKRW: totalCost * 1300 // í™˜ìœ¨ ëŒ€ëµ 1300ì›
  }
}
